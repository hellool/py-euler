Euler Technique: Recursive With Lookup

Today's discussion is the technique of recursive solutions using lookup tables, with the specific example of efficiently factoring "medium" integers into primes. Factoring truly large integers is of course a non-trivial problem which is sufficiently trusted to be difficult that it is used as the basis for RSA among other cryptographic schemes. When I say "medium" integers I refer to the range of integers that is often used in PE problems of this nature -- small enough that an algorithm will be fast if the correct technique is used, but large enough to be prohibitively difficult to process using a less refined method. What this range is exactly can vary dramatically based on the problem in question, and one can often get a sense from the magnitude of the numbers in the problems just how efficient the intended solution will need to be. In our case I will consider the fairly common use case of a problem where we need to factor a significant fraction of all of the numbers less than N. 

A common iterative first approach would be to, for each number x from 1 to N, loop over the prime numbers less than the square root of x and check for divisibility. Disregarding the varying density of primes for now, we could then say that factoring x has a complexity of O(x^(1/2)), and factoring the numbers from 1 to N would have an overall complexity of O(N^(3/2)). That might not seem so bad if N is on the order of say, 10^5 or 10^6, and indeed I used this method successfully on some of the earlier problems. However, the PE designers are well aware of the speed of a brute force method and, in later problems, have scaled the requirements for a solution such that this approach is not feasible unless you are very patient and have access to computational resources significantly more powerful than a typical PC. Needless to say, brute forcing an inefficient solution, even if it will work for a given problem, is not in the spirit of PE, so we endeavor to refine our understanding of the math and algorithms involved. 

The first thing we might ask ourselves is: can we take advantage of what we've already computed to factor x faster?
To state this question more generally, for a function f(x) whose computation requires several pieces, would any of those pieces have already been computed from a previous iteration (with a number smaller than x)? If so, is there a way we could reasonably create a lookup table that would reduce the number of operations needed to calculate f(x)? Lets see.

If the prime factorization of x is: x = p1^m1 * p2^m2 * ... * pn^mn, with p1 < p2 < ... < pn, then we will discover p1 first when iterating over the primes. Since we are doing this for every number smaller than x as well, then (x / p1) is a number that we have already factored, with a prime factorization of (x / p1) = p1^(m1-1) * p2^m2 * ... * pn^mn, and so factoring x should be as simple as finding its smallest prime factor p1, looking up the factorization of (x / p1), and appending p1 to this. For most of the composite numbers we are interested in, the smallest prime factor will be quite small relative to the square root, so this is, on average, a way faster solution. 

The second question we need to ask is about storage. In this case it's pretty easy to see that our lookup table only needs to have, at most, one element for each x smaller than N, and that element is a list of prime factors which will not scale quickly as x grows. So our storage space will scale roughly linearly with N, good! You'll have to be careful if it's not as obvious how the size of the table can scale. If say you need a table of a two dimensional function f(a,b) for a recursive formula (fairly common in PE), the lookup table can end up being far too large to be viable. Sometimes you can get by using a specific subset of your domain in the lookup table, with the understanding that certain, usually smaller, inputs will be used far more often than the larger ones in recursive solutions and so much of the domain can be omitted without a significant drop in speed. For example if I were really concerned with the size of my lookup table I might decide to only store the factorizations of numbers smaller than sqrt(N) instead of N. For larger N we will usually get below this threshold after dividing by a few small factors, so this table is still quite useful, trading only a bit of efficiency for a much smaller footprint in memory.  But more often if you are forced into a situation of needing a very large lookup table it means there is likely an important trick or nugget of information that you are still missing for a fast solution. 

With regards to the factorization problem, an algorithm involving a lookup table might go something like this --
let F(x) be the function that computes the prime factorization of x, and L be a lookup table such that if x is in L, F(x) = L[x]
let N be the maximum value that needs to be factored.
let P be a list of prime numbers less than N. (ideally a structure with a O(1) check for membership such a Python dict or set)

for x from 1 to N:
compute F(x)

to compute F(x):
	if x is in L, return L[x]
	if x is prime, return {x} as the factorization (it is not necessary to add primes to L)
	otherwise, 
	iterate over P until we find any prime, p1 that divides x (likely the smallest one)
	find m the largest integer such that p1^m divides x
	let rem = (x / p1^m)
	if rem is in L, then we get {p1^m} appended to L[rem]. (In this case, since rem < x, it should be in L, or prime.)
	otherwise, compute F(rem) recursively (trivial if rem turns out to be prime), then append {p1^m}
	add x to L with this factorization, then return it.
	
So just how much faster is this than the iterative solution? Well, consider that now instead of needing to check a number of primes roughly proportional to sqrt(x), We only need to check up through the smallest prime factor of x. Although there are clearly numbers for which this is equal or nearly the square root (for example, if x is a prime squared or the product of a pair of twin primes), on average it is far smaller. How much smaller? I was curious, so I wrote a simple script to find the index of the smallest prime factor in the prime list for each number from 1 to N and the index of the largest prime less than its square root. By taking the ratio of these indices we can calculate the amount of iterations saved over the first solution. For 1 to 10^5, the average ratio is ~0.083 and for 10^6 we get ~0.048. So if our target N were 1 million, we would expect the recursive solution with lookup table to require checking only about 5% of the factors that we check in the iterative solution, and this only gets better as N gets larger. It could still be refined, but it's good progress! 